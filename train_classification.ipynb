{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and installations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Packages\n",
    "\"\"\"\n",
    "Install necessary packages for XLM-RoBERTa fine-tuning\n",
    "Run this cell first to ensure all dependencies are available\n",
    "\"\"\"\n",
    "\n",
    "# !pip install transformers datasets torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install accelerate evaluate scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import Libraries and Setup\n",
    "\"\"\"\n",
    "Import all necessary libraries and set up the environment\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    XLMRobertaTokenizer, \n",
    "    XLMRobertaForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "### TODO: set the seed\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "### TODO: set the device\n",
    "# DEVICE = \"0\"\n",
    "# DEVICE = \"1\"\n",
    "# DEVICE = \"2\"\n",
    "# DEVICE = \"3\"\n",
    "DEVICE = \"-1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = DEVICE\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Configuration and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions to load and preprocess your dataset\n",
    "Modify the load_data function according to your data format\n",
    "\"\"\"\n",
    "\n",
    "def load_data(data_path: str, text_column: str = 'text', label_column: str = 'label') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dataset from various formats (CSV, JSON, TSV)\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to your dataset file\n",
    "        text_column (str): Name of the column containing text data\n",
    "        label_column (str): Name of the column containing labels\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Detect file format and load accordingly\n",
    "    if data_path.endswith('.csv'):\n",
    "        df = pd.read_csv(data_path)\n",
    "    elif data_path.endswith('.json') or data_path.endswith('.jsonl'):\n",
    "        df = pd.read_json(data_path, lines=data_path.endswith('.jsonl'))\n",
    "    elif data_path.endswith('.tsv'):\n",
    "        df = pd.read_csv(data_path, sep='\\t')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use CSV, JSON, JSONL, or TSV\")\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Text column '{text_column}' not found in dataset\")\n",
    "    if label_column not in df.columns:\n",
    "        raise ValueError(f\"Label column '{label_column}' not found in dataset\")\n",
    "    \n",
    "    # Clean data\n",
    "    df = df.dropna(subset=[text_column, label_column])\n",
    "    df[text_column] = df[text_column].astype(str)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} samples\")\n",
    "    print(f\"Unique labels: {df[label_column].unique()}\")\n",
    "    print(f\"Label distribution:\\n{df[label_column].value_counts()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_label_mapping(labels: List[str]) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Create bidirectional mapping between string labels and integers\n",
    "    \n",
    "    Args:\n",
    "        labels (List[str]): List of unique string labels\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], Dict[int, str]]: (label_to_id, id_to_label) mappings\n",
    "    \"\"\"\n",
    "    unique_labels = sorted(list(set(labels)))\n",
    "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "    \n",
    "    print(f\"Label mapping: {label_to_id}\")\n",
    "    return label_to_id, id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure your dataset paths and parameters\n",
    "Modify these variables according to your setup\n",
    "\"\"\"\n",
    "\n",
    "# Dataset Configuration\n",
    "#### TODO : Update these paths and column names based on your dataset \n",
    "DATA_PATH = \"/path/to/your/dataset.csv\"\n",
    "TEXT_COLUMN = \"text\"  # Name of column containing text\n",
    "LABEL_COLUMN = \"label\"  # Name of column containing labels\n",
    "\n",
    "\n",
    "print(\"Loading your dataset...\")\n",
    "df = load_data(DATA_PATH, TEXT_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "# Create label mappings\n",
    "labels = df[LABEL_COLUMN].tolist()\n",
    "label_to_id, id_to_label = create_label_mapping(labels)\n",
    "num_labels = len(label_to_id)\n",
    "\n",
    "# Convert string labels to integer IDs\n",
    "df['label_id'] = df[LABEL_COLUMN].map(label_to_id)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Number of classes: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the dataset into train, validation, and test sets\n",
    "\"\"\"\n",
    "\n",
    "# Split configuration\n",
    "TRAIN_SIZE = 0.8\n",
    "VAL_SIZE = 0.1\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Ensure splits add up to 1.0\n",
    "assert abs(TRAIN_SIZE + VAL_SIZE + TEST_SIZE - 1.0) < 1e-6, \"Split sizes must sum to 1.0\"\n",
    "\n",
    "# First split: separate test set\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=SEED, \n",
    "    stratify=df['label_id']\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, \n",
    "    test_size=VAL_SIZE/(TRAIN_SIZE + VAL_SIZE), \n",
    "    random_state=SEED, \n",
    "    stratify=train_val_df['label_id']\n",
    ")\n",
    "\n",
    "print(\"Data splits:\")\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"Validation: {len(val_df)} samples\") \n",
    "print(f\"Test: {len(test_df)} samples\")\n",
    "\n",
    "# Verify label distribution in each split\n",
    "print(\"\\nLabel distribution across splits:\")\n",
    "for split_name, split_df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    dist = split_df[LABEL_COLUMN].value_counts().sort_index()\n",
    "    print(f\"{split_name}: {dict(dist)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Initialize XLM-RoBERTa model and tokenizer\n",
    "\"\"\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"xlm-roberta-base\"  # XLM-RoBERTa Base (~125M parameters)\n",
    "MAX_LENGTH = 512  # Maximum sequence length for tokenization\n",
    "\n",
    "print(f\"Loading model and tokenizer: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load model for sequence classification\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id,\n",
    "    ignore_mismatched_sizes=True  # In case of size mismatches\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Tokenize the text data for training\n",
    "\"\"\"\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text examples for the model\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of examples from the dataset\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Tokenized inputs\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples[TEXT_COLUMN],\n",
    "        truncation=True,\n",
    "        padding=False,  # Will be handled by data collator\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = HFDataset.from_pandas(train_df[[TEXT_COLUMN, 'label_id']])\n",
    "val_dataset = HFDataset.from_pandas(val_df[[TEXT_COLUMN, 'label_id']])\n",
    "test_dataset = HFDataset.from_pandas(test_df[[TEXT_COLUMN, 'label_id']])\n",
    "\n",
    "# Rename label_id column to labels (required by transformers)\n",
    "train_dataset = train_dataset.rename_column('label_id', 'labels')\n",
    "val_dataset = val_dataset.rename_column('label_id', 'labels')\n",
    "test_dataset = test_dataset.rename_column('label_id', 'labels')\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove text column (no longer needed)\n",
    "train_dataset = train_dataset.remove_columns([TEXT_COLUMN])\n",
    "val_dataset = val_dataset.remove_columns([TEXT_COLUMN])\n",
    "test_dataset = test_dataset.remove_columns([TEXT_COLUMN])\n",
    "\n",
    "print(\"Tokenization completed!\")\n",
    "print(f\"Training set features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Arguments Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure training parameters\n",
    "Full documentation of all available arguments\n",
    "\"\"\"\n",
    "\n",
    "# Output directory for model checkpoints and logs\n",
    "OUTPUT_DIR = \"./xlm-roberta-finetuned\"\n",
    "\n",
    "# Training Arguments with full documentation\n",
    "training_args = TrainingArguments(\n",
    "    # === OUTPUT AND LOGGING ===\n",
    "    output_dir=OUTPUT_DIR,                    # Directory to save model checkpoints and logs\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",         # Directory for TensorBoard logs\n",
    "    logging_steps=10,                         # Log training metrics every N steps\n",
    "    logging_strategy=\"steps\",                 # When to log (\"steps\" or \"epoch\")\n",
    "    \n",
    "    # === TRAINING CONFIGURATION ===\n",
    "    num_train_epochs=3,                       # Number of training epochs\n",
    "    per_device_train_batch_size=8,            # Batch size per GPU for training\n",
    "    per_device_eval_batch_size=16,            # Batch size per GPU for evaluation\n",
    "    gradient_accumulation_steps=2,            # Accumulate gradients over N steps (effective batch size = batch_size * gradient_accumulation_steps * num_gpus)\n",
    "    \n",
    "    # === LEARNING RATE AND OPTIMIZATION ===\n",
    "    learning_rate=2e-5,                       # Learning rate for AdamW optimizer\n",
    "    weight_decay=0.01,                        # Weight decay for regularization\n",
    "    adam_beta1=0.9,                          # Beta1 parameter for AdamW\n",
    "    adam_beta2=0.999,                        # Beta2 parameter for AdamW\n",
    "    adam_epsilon=1e-8,                       # Epsilon parameter for AdamW\n",
    "    max_grad_norm=1.0,                       # Maximum gradient norm for clipping\n",
    "    \n",
    "    # === LEARNING RATE SCHEDULING ===\n",
    "    lr_scheduler_type=\"linear\",               # Type of learning rate scheduler (\"linear\", \"cosine\", \"polynomial\", etc.)\n",
    "    warmup_steps=100,                        # Number of warmup steps for learning rate scheduler\n",
    "    # warmup_ratio=0.1,                      # Alternative: warmup as ratio of total steps\n",
    "    \n",
    "    # === EVALUATION AND SAVING ===\n",
    "    evaluation_strategy=\"steps\",              # When to evaluate (\"steps\", \"epoch\", or \"no\")\n",
    "    eval_steps=50,                           # Evaluate every N steps\n",
    "    save_strategy=\"steps\",                    # When to save checkpoints (\"steps\", \"epoch\", or \"no\")\n",
    "    save_steps=50,                           # Save checkpoint every N steps\n",
    "    save_total_limit=3,                      # Maximum number of checkpoints to keep\n",
    "    load_best_model_at_end=True,             # Load best model at end of training\n",
    "    metric_for_best_model=\"eval_accuracy\",    # Metric to use for selecting best model\n",
    "    greater_is_better=True,                   # Whether higher metric values are better\n",
    "    \n",
    "    # === EARLY STOPPING ===\n",
    "    # early_stopping_patience=3,             # Stop training if metric doesn't improve for N evaluations\n",
    "    # early_stopping_threshold=0.001,        # Minimum improvement to reset patience counter\n",
    "    \n",
    "    # === HARDWARE AND PERFORMANCE ===\n",
    "    fp16=torch.cuda.is_available(),          # Use mixed precision training (faster on modern GPUs)\n",
    "    # bf16=False,                            # Use bfloat16 (alternative to fp16, better on A100/H100)\n",
    "    dataloader_num_workers=4,                # Number of subprocesses for data loading\n",
    "    dataloader_pin_memory=True,              # Pin memory for faster GPU transfer\n",
    "    \n",
    "    # === REPRODUCIBILITY ===\n",
    "    seed=SEED,                                 # Random seed for reproducibility\n",
    "    data_seed=SEED,                           # Seed for data shuffling\n",
    "    \n",
    "    # === REPORTING AND INTEGRATION ===\n",
    "    report_to=[\"tensorboard\"],               # Reporting tools (\"tensorboard\", \"wandb\", \"comet_ml\", etc.)\n",
    "    run_name=\"xlm-roberta-classification\",   # Name for this training run\n",
    "    \n",
    "    # === ADVANCED TRAINING OPTIONS ===\n",
    "    remove_unused_columns=True,              # Remove dataset columns not used by the model\n",
    "    label_names=[\"labels\"],                  # Names of the label columns\n",
    "    # group_by_length=True,                  # Group samples by length to minimize padding\n",
    "    # length_column_name=\"length\",           # Column name for sequence lengths\n",
    "    \n",
    "    # === CHECKPOINTING AND RESUMING ===\n",
    "    # resume_from_checkpoint=None,           # Path to checkpoint to resume from\n",
    "    ignore_data_skip=False,                  # Skip data loading optimizations when resuming\n",
    "    \n",
    "    # === DISTRIBUTED TRAINING ===\n",
    "    # local_rank=-1,                         # Local rank for distributed training\n",
    "    # ddp_find_unused_parameters=False,      # Find unused parameters in DDP\n",
    "    # ddp_bucket_cap_mb=25,                  # DDP bucket size in MB\n",
    "    \n",
    "    # === PREDICTION AND GENERATION ===\n",
    "    predict_with_generate=False,             # Use generation for predictions (not applicable for classification)\n",
    "    \n",
    "    # === PUSH TO HUB ===\n",
    "    # push_to_hub=False,                     # Push model to Hugging Face Hub\n",
    "    # hub_model_id=\"your-username/model-name\", # Model name on the Hub\n",
    "    # hub_token=None,                        # Hugging Face token for authentication\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Total training steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Define evaluation metrics for model performance\n",
    "\"\"\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the model\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Predictions and labels from the model\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Dictionary of computed metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Get predicted class (highest probability)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }\n",
    "    \n",
    "    # Add per-class metrics\n",
    "    for i, label_name in id_to_label.items():\n",
    "        if i < len(precision_per_class):  # Ensure index is valid\n",
    "            metrics[f'f1_{label_name}'] = f1_per_class[i]\n",
    "            metrics[f'precision_{label_name}'] = precision_per_class[i]\n",
    "            metrics[f'recall_{label_name}'] = recall_per_class[i]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"Evaluation metrics and data collator configured!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Trainer and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Initialize the Trainer and begin fine-tuning\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Start training\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Save label mappings\n",
    "with open(f\"{OUTPUT_DIR}/label_mappings.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"label_to_id\": label_to_id,\n",
    "        \"id_to_label\": {str(k): v for k, v in id_to_label.items()}  # JSON keys must be strings\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate the fine-tuned model on the test set\n",
    "\"\"\"\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
    "\n",
    "print(\"Test Results:\")\n",
    "print(\"=\" * 30)\n",
    "for key, value in test_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "class_names = [id_to_label[i] for i in range(len(id_to_label))]\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class performance\n",
    "print(\"\\nPer-class Performance:\")\n",
    "print(\"=\" * 30)\n",
    "precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=None\n",
    ")\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    if i < len(precision_per_class):\n",
    "        print(f\"{class_name}:\")\n",
    "        print(f\"  Precision: {precision_per_class[i]:.4f}\")\n",
    "        print(f\"  Recall: {recall_per_class[i]:.4f}\")\n",
    "        print(f\"  F1-Score: {f1_per_class[i]:.4f}\")\n",
    "        print(f\"  Support: {support[i]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Function and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create inference function and test with new examples\n",
    "\"\"\"\n",
    "\n",
    "def predict_text(text: str, model, tokenizer, device, label_mapping):\n",
    "    \"\"\"\n",
    "    Predict the class of a single text input\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to classify\n",
    "        model: Fine-tuned model\n",
    "        tokenizer: Model tokenizer\n",
    "        device: Device to run inference on\n",
    "        label_mapping: Dictionary mapping class IDs to labels\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Prediction results with probabilities\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Convert to numpy and get results\n",
    "    predictions = predictions.cpu().numpy()[0]\n",
    "    predicted_class_id = np.argmax(predictions)\n",
    "    predicted_class = label_mapping[predicted_class_id]\n",
    "    confidence = predictions[predicted_class_id]\n",
    "    \n",
    "    # Get all class probabilities\n",
    "    class_probabilities = {\n",
    "        label_mapping[i]: prob for i, prob in enumerate(predictions)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"predicted_class\": predicted_class,\n",
    "        \"confidence\": confidence,\n",
    "        \"all_probabilities\": class_probabilities\n",
    "    }\n",
    "\n",
    "def predict_texts(texts: List[str], model, tokenizer, device, label_mapping):\n",
    "    \"\"\"\n",
    "    Predict classes for multiple texts\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of input texts\n",
    "        model: Fine-tuned model\n",
    "        tokenizer: Model tokenizer\n",
    "        device: Device to run inference on\n",
    "        label_mapping: Dictionary mapping class IDs to labels\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: List of prediction results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        result = predict_text(text, model, tokenizer, device, label_mapping)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# Test with example texts\n",
    "test_texts = [\n",
    "    \"This product is absolutely amazing! I love it!\",\n",
    "    \"Worst purchase ever. Complete waste of money.\",\n",
    "    \"It's okay, nothing special but not bad either.\",\n",
    "    \"¡Este producto es increíble! Lo recomiendo totalmente.\",\n",
    "    \"Ce produit est vraiment décevant.\"\n",
    "]\n",
    "\n",
    "print(\"Testing inference with example texts:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "predictions = predict_texts(test_texts, model, tokenizer, device, id_to_label)\n",
    "\n",
    "for pred in predictions:\n",
    "    print(f\"Text: {pred['text']}\")\n",
    "    print(f\"Predicted: {pred['predicted_class']} (confidence: {pred['confidence']:.4f})\")\n",
    "    print(\"All probabilities:\")\n",
    "    for class_name, prob in pred['all_probabilities'].items():\n",
    "        print(f\"  {class_name}: {prob:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading and Deployment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Functions to load the saved model for deployment\n",
    "\"\"\"\n",
    "\n",
    "def load_trained_model(model_path: str, device: str = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Load the fine-tuned model from saved checkpoint\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model directory\n",
    "        device (str): Device to load the model on\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (model, tokenizer, label_mappings)\n",
    "    \"\"\"\n",
    "    # Load tokenizer\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Load model\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(model_path)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load label mappings\n",
    "    with open(f\"{model_path}/label_mappings.json\", \"r\") as f:\n",
    "        mappings = json.load(f)\n",
    "        label_to_id = mappings[\"label_to_id\"]\n",
    "        id_to_label = {int(k): v for k, v in mappings[\"id_to_label\"].items()}\n",
    "    \n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    print(f\"Available classes: {list(label_to_id.keys())}\")\n",
    "    \n",
    "    return model, tokenizer, id_to_label, label_to_id\n",
    "\n",
    "# Example of loading the model (uncomment to test)\n",
    "# loaded_model, loaded_tokenizer, loaded_id_to_label, loaded_label_to_id = load_trained_model(OUTPUT_DIR, device)\n",
    "\n",
    "print(\"Model loading function created!\")\n",
    "print(f\"To load your trained model later, use: load_trained_model('{OUTPUT_DIR}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display training summary and provide next steps\n",
    "\"\"\"\n",
    "\n",
    "print(\"🎉 TRAINING COMPLETED SUCCESSFULLY! 🎉\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n📊 TRAINING SUMMARY:\")\n",
    "print(f\"• Model: XLM-RoBERTa Base (~125M parameters)\")\n",
    "print(f\"• Training samples: {len(train_dataset)}\")\n",
    "print(f\"• Validation samples: {len(val_dataset)}\")\n",
    "print(f\"• Test samples: {len(test_dataset)}\")\n",
    "print(f\"• Number of classes: {num_labels}\")\n",
    "print(f\"• Classes: {list(label_to_id.keys())}\")\n",
    "print(f\"• Training epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"• Final test accuracy: {test_results['test_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n💾 SAVED FILES:\")\n",
    "print(f\"• Model directory: {OUTPUT_DIR}\")\n",
    "print(f\"• Model files: pytorch_model.bin, config.json\")\n",
    "print(f\"• Tokenizer files: tokenizer.json, vocab.json\")\n",
    "print(f\"• Label mappings: label_mappings.json\")\n",
    "print(f\"• Training logs: {OUTPUT_DIR}/logs/\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
